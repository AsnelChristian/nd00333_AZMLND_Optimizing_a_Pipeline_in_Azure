# Optimizing an ML Pipeline in Azure

## Overview
This project is part of the Udacity Azure ML Nanodegree.
In this project, we build and optimize an Azure ML pipeline using the Python SDK and a provided Scikit-learn model.
This model is then compared to an Azure AutoML run.

## Summary
**In 1-2 sentences, explain the problem statement: e.g "This dataset contains data about... we seek to predict..."**

This dataset contains data about a marketing campaign initiated by a bank, several information about the customers that
were reached out concerning the campaign are provided, information like when last they were contacted, how long the 
conversation lasted, their marital status etc... With this data we seek to predict how likely the customer are to
subscribe to the product being advertised.

**In 1-2 sentences, explain the solution: e.g. "The best performing model was a ..."**


## Scikit-learn Pipeline
**Explain the pipeline architecture, including data, hyperparameter tuning, and classification algorithm.**
We use the LogicRegression algorithm provided by scikit-learn to train and test the data. To accomplish this
- We first clean the data and extract the label into a different dataframe.
- We split the data into the test and training samples
- We setup hyperparameter tuning to determine the best possible value for the ``inverse of regularization strength`` parameter of the logic regression algorithm from the uniform space between 0.5 and 2.0
- We setup the hyperparameter tuning to stop  


**What are the benefits of the parameter sampler you chose?**
We use a Random sampler for the hypertuning. This sampler is good because it first supports early termination policies and it allows to pick value over a continuous range. In our case we want to pick the optimal for the ``inverse of regularization strength`` to squeeze out the last drop of performance of the logic regression algorithm. It is a floating point value so we need a continuous range.

**What are the benefits of the early stopping policy you chose?**
The BanditPolicy used to stop the hyperparemeter tuning helps to stop the process when there is a considerable drop in
``Accuracy``(metric being checked against). We stop the process when the accuracy drop by ``0.2`` after 10 runs.

## AutoML
**In 1-2 sentences, describe the model and hyperparameters generated by AutoML.**

## Pipeline comparison
**Compare the two models and their performance. What are the differences in accuracy? In architecture? If there was a difference, why do you think there was one?**

## Future work
**What are some areas of improvement for future experiments? Why might these improvements help the model?**
To improve on the model some more data cleaning could be done. Based on the importance of each column in the decision making
of the best performing model, we could remove those that do not really affect it. 

## Proof of cluster clean up
**If you did not delete your compute cluster in the code, please complete this section. Otherwise, delete this section.**
**Image of cluster marked for deletion**
